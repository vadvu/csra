<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="csra">
<title>Materials for CSRA students/interns • csra</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Materials for CSRA students/interns">
<meta property="og:description" content="csra">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light" data-bs-theme="light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">csra</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.2</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/csra.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/student.html">Materials for CSRA students/interns</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Materials for CSRA students/interns</h1>
            
      
      
      <div class="d-none name"><code>student.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="some-r">Some R<a class="anchor" aria-label="anchor" href="#some-r"></a>
</h2>
<ol style="list-style-type: decimal">
<li>Rstudio <a href="https://ucdavis-bioinformatics-training.github.io/Oct2017-ILRI-Workshop/Cheat_Sheets/rstudio-IDE-cheatsheet.pdf" class="external-link">Cheatsheet</a>.<br>
</li>
<li>Good practical books/resources on statistics in R:</li>
</ol>
<ul>
<li>
<a href="https://book.stat420.org/" class="external-link">1</a> - huge handbook with lots
of examples and explanations</li>
<li>
<a href="https://stats.oarc.ucla.edu/other/dae/" class="external-link">2</a> - more
practical and concise</li>
<li>
<a href="https://methodenlehre.github.io/intro-to-rstats/index.html" class="external-link">3</a>
- <em>sociological</em> statistics</li>
<li>
<a href="https://murraylax.org/rtutorials/" class="external-link">4</a> - short handbook
with lots of topics</li>
<li>
<a href="https://evalf21.classes.andrewheiss.com/example/" class="external-link">5</a> -
more on causality</li>
<li>
<a href="https://bookdown.org/mike/data_analysis/" class="external-link">5+</a> - more
theory and math under statistics<br>
</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Visualization in R: <a href="https://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html" class="external-link">1</a>,
<a href="https://r-coder.com/r-graphs/" class="external-link">2</a>, <a href="https://r-statistics.co/ggplot2-Tutorial-With-R.html" class="external-link">3</a>.</li>
<li>Research Design (must read at least 1-10 chapters to understand why
we use statistic): <a href="https://theeffectbook.net/index.html" class="external-link">1</a>
</li>
<li>
<a href="https://online.stat.psu.edu/stat462/node/207/" class="external-link">Intro</a> to
logistic regression</li>
</ol>
<p>Also note extremely powerful package <a href="https://strengejacke.github.io/sjPlot/" class="external-link"><code>sjPlot</code></a>
that provides you with regression tables, marginal effects, contingency
tables and etc.</p>
</div>
<div class="section level2">
<h2 id="basics">Basics<a class="anchor" aria-label="anchor" href="#basics"></a>
</h2>
<p><strong>Statistical inference?</strong> It is a conclusion from
sample about population based on estimates from a model. Statistical
estimation types are:</p>
<ol style="list-style-type: decimal">
<li>Interval (like confidence intervals)</li>
<li>Point (like mean)</li>
</ol>
<div class="section level3">
<h3 id="notation-and-basic-concepts">Notation and basic concepts:<a class="anchor" aria-label="anchor" href="#notation-and-basic-concepts"></a>
</h3>
<div class="section level4">
<h4 id="expectation-ex-properties">Expectation <span class="math inline">\(E(X)\)</span>
properties<a class="anchor" aria-label="anchor" href="#expectation-ex-properties"></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(E(c)=c\)</span>, where <span class="math inline">\(c\)</span> - <em>const</em> <br>
</li>
<li>
<span class="math inline">\(E(cX)=c*E(X)\)</span> <br>
</li>
<li>
<span class="math inline">\(E(X+Y)=E(X)+E(Y)\)</span> <br>
</li>
<li>if <em>X</em> and <em>Y</em> are <em>independent</em>: <span class="math inline">\(E(XY)=E(X)*E(Y)\)</span> </li>
</ol>
</div>
<div class="section level4">
<h4 id="variance-dxvarx-properties">Variance <span class="math inline">\(D(X)=Var(X)\)</span>
properties<a class="anchor" aria-label="anchor" href="#variance-dxvarx-properties"></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(D(X)=E([X-E(X)]^2)=E(X^2)-E^2(X)\)</span> <br>
</li>
<li>
<span class="math inline">\(D(X)\geq 0\)</span> <br>
</li>
<li>
<span class="math inline">\(D(c)=0\)</span> <br>
</li>
<li>
<span class="math inline">\(D(cX)=c^2D(X)\)</span> <br>
</li>
<li>
<span class="math inline">\(D(X+c)=D(X)+0\)</span> </li>
</ol>
</div>
<div class="section level4">
<h4 id="covariance-covxy-properties">Covariance <span class="math inline">\(Cov(X,Y)\)</span>
properties<a class="anchor" aria-label="anchor" href="#covariance-covxy-properties"></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(Cov(X,Y)=E[(X-E(X))*(Y-E(Y))]=E(XY)-E(X)E(Y)\)</span> <br>
</li>
<li>
<span class="math inline">\(Cov(X,X)=D(X)\)</span> <br>
</li>
<li>
<span class="math inline">\(Cov(X,Y)=Cov(Y,X)\)</span> <br>
</li>
<li>
<span class="math inline">\(Cov(X+Y,Z)=Cov(X,Z)+Cov(X,Z)\)</span> <br>
</li>
<li>
<span class="math inline">\(Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)\)</span> <br>
</li>
<li>
<span class="math inline">\(Cov(cX,Y)=c*Cov(X,Y)\)</span> <br>
</li>
<li>
<span class="math inline">\(Cov(X,c)=0\)</span> <br>
</li>
<li>
<span class="math inline">\(Cov(X+c,Y)=Cov(X,Y)\)</span> <br>
</li>
<li>
<span class="math inline">\(D(X \pm Y)=D(X)\pm
2Cov(X,Y)+D(Y)\)</span> <br>
</li>
<li>if <em>X</em> and <em>Y</em> are <em>independent</em>: <span class="math inline">\(Cov(X,Y)=0\)</span> and <span class="math inline">\(D(X \pm Y)=D(X)\pm D(Y)\)</span> </li>
</ol>
</div>
<div class="section level4">
<h4 id="correlation-corrxy-properties">Correlation <span class="math inline">\(Corr(X,Y)\)</span>
properties<a class="anchor" aria-label="anchor" href="#correlation-corrxy-properties"></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}\)</span> <br>
</li>
<li>
<span class="math inline">\(|Corr(X,Y)|\leq 1\)</span> <br>
</li>
<li>if <em>X</em> and <em>Y</em> are <em>independent</em> (linearly):
<span class="math inline">\(Corr(X,Y)=0\)</span> <br>
</li>
<li>
<span class="math inline">\(Corr(X,X)=1\)</span> <br>
</li>
<li>
<span class="math inline">\(Corr(X,Y)=1\)</span> if and only if
there exist values <span class="math inline">\(a\neq 0\)</span> and
<span class="math inline">\(b\)</span> such that <span class="math inline">\(Y=aX+b\)</span> <br>
</li>
<li>
<span class="math inline">\(Corr(X+c,Y)=Corr(X,Y)\)</span> <br>
</li>
<li>
<span class="math inline">\(Corr(X*(\pm c),Y)=\pm Corr(X,Y)\)</span>
and if <span class="math inline">\(c=0:Corr(cX,Y)=0\)</span> </li>
</ol>
</div>
<div class="section level4">
<h4 id="covariance-matrix">Covariance matrix<a class="anchor" aria-label="anchor" href="#covariance-matrix"></a>
</h4>
<p>Assume we have matrix <span class="math inline">\(X\)</span> where
<span class="math inline">\(x_{ij}\)</span> is a random variable: <span class="math display">\[\begin{bmatrix}
    x_{11} &amp; \dots &amp; x_{1j} \\
    \vdots &amp; \ddots &amp; \vdots \\
    x_{i1} &amp; \dots &amp; x_{ij} \\
\end{bmatrix}\]</span></p>
<p>The <strong>expectation</strong> of that matrix <span class="math inline">\(E(X)\)</span> is: <span class="math display">\[\begin{bmatrix}
    E(x_{11}) &amp; \dots &amp; E(x_{1j}) \\
    \vdots &amp; \ddots &amp; \vdots \\
    E(x_{i1}) &amp; \dots &amp; E(x_{ij}) \\
\end{bmatrix}\]</span></p>
<p><strong>Properties of <span class="math inline">\(E(X)\)</span> in
matrix</strong>:<br>
1. if <span class="math inline">\(B=[b_1, ..., b_n]^T\)</span> is a
constant-vector, then <span class="math inline">\(E(B)=B\)</span> 2.
<span class="math inline">\(E(cX)=cE(X)\)</span>, if <span class="math inline">\(c\)</span> - some constant term<br>
3. <span class="math inline">\(E(X+Y)=E(X)+E(Y)\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables vectors<br>
4. <span class="math inline">\(E(AX)=AE(X)\)</span> if <span class="math inline">\(A\)</span> - matrix with constant terms</p>
<p>The <strong>covariance</strong> of random vector <span class="math inline">\(X=[x_1,...,x_i]^T\)</span> is <span class="math inline">\(V(X)\)</span>: <span class="math display">\[V(X)=\begin{bmatrix}
    Cov(x_1, x_1) &amp; \dots &amp; Cov(x_1,x_i) \\
    \vdots &amp; \ddots &amp; \vdots \\
    Cov(x_i, x_1) &amp; \dots &amp; Cov(x_i, x_i) \\
\end{bmatrix}= \begin{bmatrix}
    Var(x_1) &amp; \dots &amp; Cov(x_1,x_i) \\
    \vdots &amp; \ddots &amp; \vdots \\
    Cov(x_i, x_1) &amp; \dots &amp; Var(x_i) \\
\end{bmatrix}\]</span> It is <em>quadratic symmetric matrix</em>. If
<span class="math inline">\(X = [X_1]\)</span> (<em>one-dimensional
random variable</em>) then <span class="math display">\[V(X)=Cov(X_1,X_1)=Var(X_1)\]</span> <span class="math inline">\(V(X)\)</span> can be written for random-vector
<span class="math inline">\(X\)</span> as: <span class="math display">\[V(X)=E[(X-E(X))*(X-E(X))^T]\]</span></p>
<p><strong>Properties of <span class="math inline">\(V(X)\)</span>
matrix</strong>:<br>
1. <span class="math inline">\(V(cX)=c^2V(X)\)</span>, where <span class="math inline">\(c\)</span> is some constant term<br>
2. <span class="math inline">\(V(X+B)=V(X)\)</span> where <span class="math inline">\(B\)</span> is constant-vector<br>
3. <span class="math inline">\(V(AX)=AV(X)A^T\)</span> where <span class="math inline">\(A\)</span> is a constant matrix<br>
4. <span class="math inline">\(Cov(cX,zX)=cV(X)z^T\)</span></p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="econometrics">Econometrics<a class="anchor" aria-label="anchor" href="#econometrics"></a>
</h2>
<div class="section level3">
<h3 id="ols-aka-linear-regression">OLS aka linear regression<a class="anchor" aria-label="anchor" href="#ols-aka-linear-regression"></a>
</h3>
<div class="section level4">
<h4 id="model">Model<a class="anchor" aria-label="anchor" href="#model"></a>
</h4>
<p>The formula for linear regression with <span class="math inline">\(k\)</span> explanatory variables is: <span class="math display">\[y_i =
\beta_0+\sum_k{\beta_kx_{i,k}}+\epsilon_i\]</span> We want to minimize
RSS (<em>Residuals Sum of Squares</em>) that is deviations of our
modeled (predicted) values of <span class="math inline">\(y\)</span>
with a hat (<span class="math inline">\(\hat y\)</span>) from observed
(real) values of <span class="math inline">\(y\)</span>. Note, <span class="math inline">\(\hat y=\beta_0+\sum_k{\beta_kx_{i,k}}\)</span>, so
we can write <em>RSS</em> as: <span class="math display">\[RSS=\sum_i{(y_i-b_1x_{i,1}-...b_kx_{i,k}})^2\]</span>
or <span class="math display">\[RSS=\sum_i{(y_i-\hat y_i)^2}\]</span>
Minimization of that sum (the quadratic amount of deviations between
modeled and observable values) can be done by taking <em>partial
derivatives</em> with respect to <span class="math inline">\(\beta_k\)</span> (lets write them more concise as
<span class="math inline">\(\theta=[\beta_1, \beta_2, ...
\beta_p]\)</span>) because they are coefficients of linear combination
of <span class="math inline">\(X\)</span> (matrix of explanatory
variables). Such a matrix has following view: <span class="math display">\[\begin{bmatrix}
    x_{11} &amp; \dots &amp; x_{1j} \\
    \vdots &amp; \ddots &amp; \vdots \\
    x_{i1} &amp; \dots &amp; x_{ij} \\
\end{bmatrix}\]</span> where columns <span class="math inline">\(j\)</span> are some variables and raws <span class="math inline">\(i\)</span> are some units. For example, units are
countries (Russia, USA, China) and variables are their population and
democracy level.</p>
<p>Turning to minimization with respect to <span class="math inline">\(\theta\)</span> parameters, we have a
<em>system</em> of <em>k</em> equations of such form (example for <span class="math inline">\(k=1\)</span>): <span class="math display">\[\sum_i{\frac{\partial RSS}{\partial
\beta_1}}=\sum_i{2*(-x_{i,1})*(y_i-\beta_1x_{i,1}-\beta_2x_{i,2}-...)}=0\]</span>
It can be rearranged (by opening brackets and dividing all equations by
<em>-2</em>) as: <span class="math display">\[\sum_i{x_{i,1}^2
\beta_1}+\sum_i{(x_{i,2}x_{i,1})\beta_2}+...=\sum_i{x_{i,1}y_i}\]</span>
In terms of <em>linear algebra</em> we have such equation: <span class="math display">\[X^TX\theta=X^Ty\]</span> where one can find <span class="math inline">\(\theta\)</span> by reformulating previous formula
as (multiply both parts by <span class="math inline">\((X^TX)^{-1}\)</span> lefty, because <span class="math inline">\(A^{-1}A=I\)</span> where <span class="math inline">\(I\)</span> is a unit matrix): <span class="math display">\[\theta = (X^TX)^{-1}X^Ty\]</span> This equation
has one solution <em>only if</em> <span class="math inline">\(det[X^TX]\ne 0\)</span> (see
<em>Assumptions</em>). Then the full model is: <span class="math display">\[y=X\theta+\epsilon\]</span> or <span class="math inline">\(y=\theta^TX^T+\epsilon\)</span></p>
</div>
<div class="section level4">
<h4 id="assumptions-gauss-markov-theorem">Assumptions (<em>Gauss-Markov theorem</em>):<a class="anchor" aria-label="anchor" href="#assumptions-gauss-markov-theorem"></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<em>Homoscedasticity</em>: <span class="math inline">\(Var(\epsilon|x)=const\)</span><br>
</li>
<li>No <em>multicollinearity</em>: if <span class="math inline">\(Corr(x_1, x_2)\neq0 \to\)</span> inflated SE, not
robust results (drop of observation will significantly change our <span class="math inline">\(\theta\)</span> estimates). In case of <em>perfect
multicollinearity</em> (<span class="math inline">\(Corr=|1|\)</span>,
or <em>dummy variable trap</em> - just opposite dummies in a model), we
cannot find estimation for <span class="math inline">\(\theta\)</span>
(because we cannot find inverse matrix of <span class="math inline">\(X^TX\)</span> due to determinant of it equals
0)<br>
</li>
<li>No <em>endogeneity</em>: <span class="math inline">\(Corr(\epsilon_i|\epsilon_j)=0\)</span> and <span class="math inline">\(E(\epsilon_i|x_{i,j})=0\)</span><br>
</li>
<li>
<em>Normality of residuals</em> and <span class="math inline">\(E(\epsilon)=0\)</span><br>
Thus, <span class="math inline">\(\epsilon_i \sim i.i.d.
N(0,\sigma^2)\)</span>, <span class="math inline">\(i.i.d\)</span> -
identically independently distributed. Therefore, residuals covariance
matrix is <span class="math inline">\(V(\epsilon)=\sigma^2I\)</span>
</li>
</ol>
</div>
<div class="section level4">
<h4 id="properties">Properties:<a class="anchor" aria-label="anchor" href="#properties"></a>
</h4>
<p>If all assumptions are held, than OLS estimation is <em>Best Linear
Unbiased Estimator</em> (<em>BLUE</em>). So, it is:<br>
1. <em>efficient</em>: min <span class="math inline">\(Var(\hat
\theta)\)</span> among all possible linear unbiased estimates of <span class="math inline">\(\theta\)</span><br>
2. <em>unbiased</em>: <span class="math inline">\(E(\hat
\theta)=\theta\)</span><br>
3. <em>consistent</em>:<br><span class="math display">\[\lim_{n\to
\inf}{\sigma^2(X^{(n)T}X^{(n)})^{-1}_{jj}}=0\]</span> or <span class="math display">\[\lim_{n\to \inf}{Var(\hat
\theta_i^{(n)})}=0\]</span> and because <span class="math inline">\(E(\hat \theta_i^{(n)})=\theta_i\)</span> we can
just write: <span class="math display">\[\lim_{n\to inf}{(\hat
\theta^{(n)}_i-\theta_i)}=0\]</span></p>
</div>
<div class="section level4">
<h4 id="standard-errors-and-significance">Standard errors and significance:<a class="anchor" aria-label="anchor" href="#standard-errors-and-significance"></a>
</h4>
<p>For identifying where <span class="math inline">\(\hat{\theta}\)</span> are significant we should
calculate its variance <span class="math inline">\(Var(\hat{\theta})\)</span>. Lets find covariance
matrix where diagonal elements are <span class="math inline">\(Cov(\theta_i,\theta_i)=Var(\theta_i)\)</span>:
<span class="math display">\[V(\hat{\theta})=\hat{\sigma}^2(X^TX)^{-1}\]</span>where
<span class="math inline">\(\hat{\sigma}^2\)</span> is estimation of
<span class="math inline">\(Var(\hat{\epsilon})\)</span> from estimated
model <span class="math inline">\(\hat{Y}=X\hat{\theta}\)</span>. Such
estimation can be done by using such statistic as: <span class="math display">\[S^2=\frac{RSS}{n-p}=\frac{\sum_i{(y_i-\hat
y_i)^2}}{n-p}\]</span> that is unbiased estimation of <span class="math inline">\(Var(\epsilon)\)</span>, because <span class="math inline">\(E(S^2)=Var(\epsilon)\)</span>. In other words, it
is <em>Sample Variance</em> of <span class="math inline">\(\epsilon\)</span>. If we have model with only
intercept, <span class="math inline">\(S^2\)</span> is just usual sample
variance of <span class="math inline">\(y\)</span>. Then SE for
statistics calculation are just <span class="math inline">\(\sqrt{diag[V(\hat{\theta})]}\)</span>. The
statistic is <span class="math inline">\(\frac{\hat{\theta}}{se_{\hat{\theta}}}\)</span>
with known distribution (<em>t-distribution</em>). We use <span class="math inline">\(\frac{\hat{\theta}}{se_{\hat{\theta}}}\)</span>
because it simply shows: <span class="math display">\[\frac{\hat{\theta}-\theta^0}{se_{\hat{\theta}}}
\sim t(n-p)\]</span>where <span class="math inline">\(\hat
\theta^0\)</span> shows the value of <span class="math inline">\(\theta\)</span> from <span class="math inline">\(H_0\)</span>. Usually it is 0, so we have just
<span class="math inline">\(\frac{\hat{\theta}}{se_{\hat{\theta}}}\)</span>.
But it is possible to use other values of <span class="math inline">\(\hat \theta^0\)</span> to test some specific null
hypotheses. For example: <span class="math display">\[H_0: \theta =
1\]</span> Then out test statistic is (it works a little bit more
complicated, but the logic is basically like this): <span class="math display">\[\frac{\hat{\theta}-\theta^0}{se_{\hat{\theta}}}=\frac{\hat{\theta}-1}{se_{\hat{\theta}}}\]</span>
For identifying <em>confidence interval</em> of <span class="math inline">\(\hat\theta\)</span>: <span class="math display">\[\hat\theta_i-t_{1-\alpha/2}\sqrt{Var(\hat\theta_i)},
\hat\theta_i+t_{1-\alpha/2}\sqrt{Var(\hat\theta_i)}\]</span> In other
words, population parameter <span class="math inline">\(\theta\)</span>
is lying in estimated CI with <span class="math inline">\(100(1-\alpha)\)</span>% probability.</p>
</div>
<div class="section level4">
<h4 id="significance-of-the-model">Significance of the model:<a class="anchor" aria-label="anchor" href="#significance-of-the-model"></a>
</h4>
<p>One wants to test significance not of each <span class="math inline">\(\theta_i\)</span>, but of the whole model with set
of estimated coefficients <span class="math inline">\(\theta\)</span>.
Thus, joint test is needed, where null hypothesis is: <span class="math display">\[H_0: \theta_0=\theta_1=...=\theta_i=0\]</span>
Thus, alternative hypothesis is that <strong>at least one coefficient
<span class="math inline">\(\theta_i\)</span> is not zero</strong>. One
of possible approach that tests such joint <span class="math inline">\(H_0\)</span> is <em>F-test</em>. Assume we have
linear model with set of coefficients <span class="math inline">\(p\)</span> and number of observations <span class="math inline">\(n\)</span>. Then the test statistic <span class="math inline">\(F\)</span> is: <span class="math display">\[F=\frac{(RSS_{H_0}-RSS)/q}{RSS/(n-p)}=\frac{ESS/(p-1)}{RSS/(n-p)}
\sim F(p-1,n-p)\]</span> This test basically compare null model
(intercept only) and estimated model, where <span class="math inline">\(RSS\)</span> - residuals sum of squares, <span class="math inline">\(ESS\)</span> - explained sum of squares (<span class="math inline">\(TSS-RSS\)</span>). Such statistic has <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(p-1\)</span> and <span class="math inline">\(n-p\)</span> degrees of freedom if <span class="math inline">\(\epsilon_i \sim iid \ N(0,\sigma^2)\)</span>.</p>
<p>Note, the first expression can be used to test any joint hypothesis
<span class="math inline">\(H_0\)</span> for coefficients, while the
last one with <span class="math inline">\(ESS\)</span> can be used only
for <span class="math inline">\(H_0\)</span> where all coefficients
assumed to be 0. The first expression is useful, for ex., for comparing
two not null-models. Assume we have a model: <span class="math display">\[y=\theta_0+\theta_1x_i+\theta_2k_i +
\theta_3z_i+\epsilon_i\]</span> And want to test hypothesis that: <span class="math display">\[H_0:\theta_2=\theta_3=0\]</span> We can use <span class="math inline">\(F\)</span> test, where <span class="math inline">\(H_0\)</span> model is <span class="math inline">\(y_i=\theta_0+\theta_1x_i+\varepsilon_i\)</span>.
Then parameter <span class="math inline">\(q\)</span> is a number of
tested null coefficients (or more precisely it is the number of linear
restrictions for the model). In this example <span class="math inline">\(q=2\)</span>.</p>
</div>
<div class="section level4">
<h4 id="endogeneity">Endogeneity<a class="anchor" aria-label="anchor" href="#endogeneity"></a>
</h4>
<p><strong>Statistical definition</strong>:<br>
1. <span class="math inline">\(E(\epsilon|X) \ne 0\)</span><br>
2. <span class="math inline">\(Corr(\epsilon, X)\ne0\)</span></p>
<p><strong>Main sources of endogeneity:</strong></p>
<ol style="list-style-type: decimal">
<li>
<em>Omitted variable problem</em>. Omitted significant variable
<span class="math inline">\(C\)</span> (appropriate control
variable)</li>
</ol>
<ul>
<li>Has effect on Y (<span class="math inline">\(C \to Y\)</span>)</li>
<li>Has effect on X (<span class="math inline">\(C \to X\)</span>)<br>
</li>
<li>
<em>Consequence</em> - biased result for <span class="math inline">\(X\)</span> if our model is: <span class="math inline">\(y=\beta_0+\beta_1x\)</span><br>
</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<em>Selection bias problem</em> (or post-treatment problem):
including of Collider variable <span class="math inline">\(Z\)</span>
(inappropriate control variable)
<ul>
<li>
<span class="math inline">\(Y \to Z\)</span><br>
</li>
<li><span class="math inline">\(X \to Z\)</span></li>
<li>No effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>
</li>
<li>
<em>Consequence</em> - biased result for <span class="math inline">\(X\)</span> if oue model is: <span class="math inline">\(y=\beta_0+\beta_1x+\beta_2z\)</span><br>
</li>
</ul>
</li>
<li><em>Reverse causality problem</em></li>
</ol>
<p><strong>Why we need controls (confounders)?</strong> Controls have
common variance (=effect) with <span class="math inline">\(X\)</span>
(independent variable) and <span class="math inline">\(Y\)</span>
(dependent variable). To get less biased estimates for <span class="math inline">\(X\)</span> we have to include controls, because
part of common variance of <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> are due to confounders, so by
including them we “clean” common variance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> from “noise” produced by other factors.
If we do not do so, “omitted variable bias” and <a href="https://mixtape.scunning.com/03-directed_acyclical_graphs" class="external-link">“backdoor”
problem</a> emerge resulting in <em>endogeneity</em> and
<em>biased</em>, <em>inconsistent</em> inference.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="panel-data">Panel data<a class="anchor" aria-label="anchor" href="#panel-data"></a>
</h2>
<div class="section level3">
<h3 id="data-types">Data types:<a class="anchor" aria-label="anchor" href="#data-types"></a>
</h3>
<ol style="list-style-type: decimal">
<li>Cross-section data - many units at one time (GDP of countries in
2002): <span class="math inline">\(y_i=y_{country}\)</span>
</li>
<li>Time-series data - one unit in time (GDP of Russia from 2002 to
2010): <span class="math inline">\(y_t=y_{year}\)</span>
</li>
<li>Panel data - units are repeated over time: <span class="math inline">\(y_{i,t}=y_{country,year}\)</span>
</li>
<li>Pooled data - panel data where researcher does not consider
<em>time</em> and <em>spatial</em> dependencies</li>
</ol>
<p><strong>Problems with panel data</strong>:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Interdependence</strong>:
<ol style="list-style-type: decimal">
<li>cross-sectional correlations (across units)</li>
<li>autocorrelation (in time)</li>
</ol>
</li>
<li>
<strong>Pooled data problem</strong>:
<ol style="list-style-type: decimal">
<li>the problem of ignoring spatial and time effects in the data is a
special case of a more general problem, that of omitting variables
(endogeneity)</li>
<li>Aggregation bias (Sympson paradox)</li>
<li>serial correlations</li>
<li>inconsistent SE <span class="math inline">\(\to\)</span> wrong
statistical inference</li>
</ol>
</li>
</ol>
</div>
<div class="section level3">
<h3 id="what-to-do">What to do?<a class="anchor" aria-label="anchor" href="#what-to-do"></a>
</h3>
<div class="section level4">
<h4 id="fixed-effects-fe">Fixed-effects (FE)<a class="anchor" aria-label="anchor" href="#fixed-effects-fe"></a>
</h4>
<p><strong>FE models</strong> can remove <em>time-invariant</em> omitted
variables, <em>unit-invariant</em> omitted variables, or both from the
variance of an outcome. This ability of FE model to remove these
confounders is a side effect of the fact that <strong>FE isolate
particular dimensions of variance in the data to analyze</strong>.</p>
<p><strong>LSDV</strong><br>
LSDV - <em>Least Squares Dummy Variables</em> model for <span class="math inline">\(N\)</span> cross-sectional/time units): <span class="math display">\[y_{i,t}=\beta_0+\gamma_1d_{1,i}+...+\gamma_{(N-1)}d_{N-1,i}+\beta_1x_{i,t}+\varepsilon_{i,t}\]</span>
where <span class="math inline">\(d\)</span> is a specific binary
variable for modeling unit-specific intercept. If set of <span class="math inline">\(d\)</span> is countries and we want include FE for
cross-sectional dimension, then <span class="math inline">\(d_{i,1}\)</span> is 1 when country is USA and 0
otherwise. Such model assumes <strong>the same coefficients for all
units</strong> but different intercepts. Intercept <span class="math inline">\(\beta_0\)</span> - average level of <span class="math inline">\(y\)</span> in the <em>reference</em>
cross-sectional/time unit. <span class="math inline">\(\gamma\)</span> -
difference in intercept between reference unit and other unit. On
average it is a difference in <span class="math inline">\(y\)</span>
between some unit and reference category (also some unit) when all other
variables being equal.<br>
Error term structure: <span class="math inline">\(\varepsilon_{i,t} =
\varepsilon_i + \varepsilon_t\)</span> (<em>cross-sectional</em>
variance + <em>time dimension</em> variance).</p>
<p><strong>What FE models show?</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Unit FE:</strong> represents the average effect of a
unit-increase in x on y as each variable <em>changes over time</em>,
generalized to all cases (<em>as GDP increases for a country over time,
how does the quality of its democracy change over time?</em>)</li>
<li>
<strong>Time FE:</strong> time FE coefficients represent the average
effect of a unit-increase in x on y as each variable <em>changes from
case to case</em>, generalized across all time points (<em>how much more
democratic are wealthier countries than poorer countries at any point in
time?</em>)</li>
</ol>
</div>
<div class="section level4">
<h4 id="random-effects-re">Random-effects (RE)<a class="anchor" aria-label="anchor" href="#random-effects-re"></a>
</h4>
<p>Instead of usual OLS equation or LSDV OLS we can use RE model with
random intercept: <span class="math display">\[y_{ij}=\beta_{0j}+\beta_1
x_{ij}+r_{ij}\]</span> <span class="math display">\[\beta_{0j}=\gamma_{00}+u_{0j}\]</span> The fist
equation refers to the <strong>first-level</strong> (individuals <span class="math inline">\(i\)</span>), while the second refers to the
<strong>second-level</strong> (clusters <span class="math inline">\(j\)</span>). <span class="math inline">\(r_{ij}\)</span> - individual-level error term,
<span class="math inline">\(u_{0j}\)</span> - cluster-level error term
that modeled both random error and cluster’s intercept at individual
level (<em>random intercept</em>). <span class="math inline">\(W_j\)</span> affects that intercept by <span class="math inline">\(\gamma_{01}\)</span> coefficient. <span class="math inline">\(\gamma_{00}\)</span> - overall intercept.</p>
<p>All estimated <strong>intercepts are assumed to be random drawings
from the same normal distribution</strong> and thus can be easily
extended to out-of-sample groups</p>
<p><strong>Model Assumptions:</strong></p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(r_{ij} \sim i.i.d. N(0,
\sigma^2)\)</span>, independent from <span class="math inline">\(X\)</span>: <span class="math inline">\(E(r_{ij}|X,W)=0 \to\)</span> <em>first-level
exogeneity</em>
</li>
<li>
<span class="math inline">\(u_{0j} \sim i.i.d. N(0,
\tau_{00})\)</span>, independent from <span class="math inline">\(X\)</span>: <span class="math inline">\(E(u_{0j}|X,W)=0 \to\)</span> <em>second-level
exogeneity</em>
</li>
<li><span class="math inline">\(Corr(r_{ij},u_{0j})=0\)</span></li>
</ol>
<p>We can combine above-mentioned equations into one: <span class="math display">\[y_{ij}=\gamma_{00}+\beta_1
x_{ij}+(u_{0j}+r_{ij})\]</span> Thus, <span class="math inline">\(\gamma_{00}\)</span> - usual intercept, <span class="math inline">\(\varepsilon_{ij}\)</span> is decomposed across two
independent source of <em>Random Variance</em> - within (individual
level) <span class="math inline">\(r_{ij}\)</span> and between (clusters
level) <span class="math inline">\(u_{0j}\)</span> clusters.</p>
</div>
<div class="section level4">
<h4 id="fe-vs--re">FE vs. RE<a class="anchor" aria-label="anchor" href="#fe-vs--re"></a>
</h4>
<p>In majority of cases FE is better then RE due to not so strict
assumptions. If all assumptions are held but one chooses FE, then
estimates are less efficient. However, if one chooses RE instead of FE
when not all assumptions are true, then estimates are biased and
inconsistent. As you have noted, there are strong assumptions about
endogeneity (individual + cluster endogeneity problem and possible
problem of <strong>omitted variables either in the first of second
levels</strong>). However, there are a lot of cases where RE are
appropriate:</p>
<ol style="list-style-type: decimal">
<li>If data has hierarchy (nested clusters) and we want to separate
within and between variance<br>
</li>
<li>There are cluster’s variables -&gt; FE might be in perfect
collinearity situation if there is just 1 year of observation (each
cluster has unique value of GDP -&gt; perfect multicollinearity)</li>
<li>If sample is fully random (surveys)<br>
</li>
<li>In other cases - FE: “<strong>without analyzing the contextual
effects and coefficient heterogeneity across higher-level
units</strong>, the ‘good old’ simple <strong>OLS regression with
cluster-robust standard errors and fixed effects</strong> at higher
levels should be retained as a valid alternative to MLM.” (Oshchepkov,
Shirokanova, 2022)</li>
<li>When <strong>size of clusters</strong> (number of observation in
each) <strong>is small</strong> ME are better and more stable</li>
</ol>
</div>
</div>
</div>
<div class="section level2">
<h2 id="limited-dv">Limited DV<a class="anchor" aria-label="anchor" href="#limited-dv"></a>
</h2>
<p>Limited Dependent Variables are variables that are not continuous, so
binary or ordinal variables with specific distributions. OLS fails to
predict them because model does not catch their distributions. In case
of binary data, OLS predicts 0.7, -0.3, 123 and other theoretically
impossible values (while there are some lovers of <em>linear probability
models</em>). Because of that, linear estimation is not appropriate and,
finally, least-squares estimation is bad approach.</p>
<div class="section level3">
<h3 id="logistic-regression">Logistic regression<a class="anchor" aria-label="anchor" href="#logistic-regression"></a>
</h3>
<p>Logistic regression models binary dependent variable using <em>Logit
link</em>. Moreover, it does it in terms of probabilities. Logistic
function is: <span class="math display">\[\pi(x)=\frac{exp(x)}{1+exp(x)}\]</span> where
<span class="math inline">\(\pi\)</span> is a probability that measures
from 0 to 1. One can see from equation that indeed <span class="math inline">\(\pi(x)\)</span> takes only positive values
(because of <span class="math inline">\(exp\)</span>) and changes from 0
to 1 (the denominator is always larger than the numerator because of the
addition of a unit). We can rewrite that function in more “regressive”
way: <span class="math display">\[\pi(X)=\frac{exp(X\theta)}{1+exp(X\theta)}=\frac{1}{1+exp(-X\theta)}\]</span>
exposing the matrix is an extremely difficult task. There is no
analytical solution for optimization, so iterative process is used with
<em>Maximum Likelihood</em> (ML) estimator <span class="math inline">\(L\)</span>: <span class="math display">\[L=\prod_i^n{\pi_i^{y_i}(1-\pi_i)^{1-y_i}}\]</span>
where <span class="math inline">\(\pi\)</span> is calculated as in
previous equation. For optimization process product function is to
sophisticated, so we make <span class="math inline">\(L\)</span> by
logarithmization to take <em>log-likelihood</em>: <span class="math display">\[l=\sum_i^n{y_iln(\pi_i)}+(1-y_i)ln(1-\pi_i)\]</span>
The estimation process is simple, but I am not going to discuss it
now.</p>
</div>
</div>
<div class="section level2">
<h2 id="rare-events-data">Rare events data<a class="anchor" aria-label="anchor" href="#rare-events-data"></a>
</h2>
<p>Revolutions, civil wars, defaults, and coups are <strong>rare
events</strong> data that can be considered as <strong>marginal
unbalanced binary data</strong> due to the strong predominance of one
class (no event). Therefore, specific methods are required to analyze
them.</p>
<p>One such method is logistic regression with a special estimator
(Firth, 1993; Kosmidis et al., 2020).</p>
<p>One more approach involves deliberately removing a significant number
of ’0’s to balance the number of observations in both classes (known as
under-sampling, discussed in Menardi and Torelli, 2014). This approach
enables the use of classical logistic regression without the concern of
imbalance.</p>
<p>As a nonparametric method, one can use a random forest model with a
quantile classifier (O’Brien &amp; Ishwaran, 2019), which is
specifically designed for classifying unbalanced data. It is important
to note that this method allows for the avoidance of assumptions about
the form of the relationship. In parametric models, authors are assumed
to make an assumption of linearity, but with this method, one can change
the form and assumption through simple operations on the variable, such
as logarithmization or representing the factor as a polynomial. This
approach can lead to reasonably accurate estimates.</p>
<p>Next, we will discuss each approach and its implemintation.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Vadim Ustyuzhanin.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
